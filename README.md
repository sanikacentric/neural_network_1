 Understanding Forward Pass, Backward Pass, Activation Functions, and Binary Classification in Neural Networks
ğŸ” 1. Forward Propagation â€“ What Happens in a Neural Network?
When data enters a neural network, it goes through the forward pass â€” this is where predictions are made.

âœ… Steps of Forward Propagation:
Linear Transformation:

ğ‘§
=
ğ‘Š
â‹…
ğ‘¥
+
ğ‘
z=Wâ‹…x+b
W = weights

x = input

b = bias

This gives us a raw output called a logit

Activation Function: Converts the logit into a meaningful output (e.g., a probability).

ğŸ”’ 2. Activation Functions â€“ Turning Numbers into Understanding
âœ… Sigmoid (Used in Binary Classification):
ğœ
(
ğ‘§
)
=
1
1
+
ğ‘’
âˆ’
ğ‘§
Ïƒ(z)= 
1+e 
âˆ’z
 
1
â€‹
 
Output is between 0 and 1

Interpreted as probability of class 1

Used in the output layer of binary classifiers

âœ… ReLU (Used in Hidden Layers):
ReLU
(
ğ‘¥
)
=
max
â¡
(
0
,
ğ‘¥
)
ReLU(x)=max(0,x)
Sets negative values to 0, passes positive values as-is

Introduces non-linearity

Helps the model learn complex patterns efficiently

Very fast and effective

ğŸ§ª 3. Loss Function â€“ Measuring the Error
Once the network produces a prediction, we need to compare it to the true label and compute how wrong it was. That's the role of the loss function.

âœ… Binary Cross Entropy (BCE):
For binary classification:

BCEÂ Loss
=
âˆ’
[
ğ‘¦
log
â¡
(
ğ‘¦
^
)
+
(
1
âˆ’
ğ‘¦
)
log
â¡
(
1
âˆ’
ğ‘¦
^
)
]
BCEÂ Loss=âˆ’[ylog( 
y
^
â€‹
 )+(1âˆ’y)log(1âˆ’ 
y
^
â€‹
 )]
Where:

ğ‘¦
y = true label (0 or 1)

ğ‘¦
^
y
^
â€‹
  = predicted probability (from sigmoid)

ğŸ” Example:
If true label is 1, and prediction is 0.9:

Loss
=
âˆ’
log
â¡
(
0.9
)
=
0.105
Loss=âˆ’log(0.9)=0.105
ğŸ”„ 4. Backward Propagation â€“ How the Model Learns
Once the loss is calculated, we do the backward pass:

PyTorch calculates how each weight in the model contributed to the error using the chain rule

Gradients are stored in .grad for each parameter

An optimizer (like SGD or Adam) updates the weights to reduce the loss

âœ… In Code:
python
Copy
Edit
loss.backward()   # computes gradients
optimizer.step()  # updates weights using gradients
ğŸ§± 5. Putting It All Together â€” A Minimal PyTorch Model
python
Copy
Edit
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, new_features):
        super().__init__()
        self.linear = nn.Linear(new_features, 1)  # Linear Layer
        self.sigmoid = nn.Sigmoid()              # Output Activation

    def forward(self, features):
        out = self.linear(features)              # z = Wx + b
        out = self.sigmoid(out)                  # y_pred = sigmoid(z)
        return out
ğŸ’¡ What It Does:
Takes input of size new_features

Applies linear transformation

Outputs a probability between 0 and 1

Perfect for binary classification tasks

ğŸ¯ Summary Table

Concept	Description
Forward Pass	Computes prediction using weights and activations
Backward Pass	Computes gradients of loss w.r.t. parameters
Sigmoid	Used in output to get probabilities
ReLU	Used in hidden layers, fast & efficient
BCE Loss	Measures error in binary classification
Model Class	Combines layers and defines prediction flow
âœ… Final Thoughts
This flow â€” linear transformation â†’ activation â†’ loss â†’ gradients â†’ update weights â€” is the heart of every deep learning model.
